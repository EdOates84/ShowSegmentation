{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import cv2\n",
    "import pickle\n",
    "import face_recognition\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ping_telegram(text):\n",
    "    import telegram\n",
    "    bot = telegram.Bot(token=\"848617644:AAH-YU71Klu7amhz0wtVBto0ACxhDhvYTaE\")\n",
    "    bot.send_message(chat_id=\"625772042\", text = text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec2HMS(seconds):\n",
    "    return time.strftime('%H:%M:%S', time.gmtime(seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeFaces(vid_path):\n",
    "    \"\"\"stores all faces from the video sorted\n",
    "       by their time of appearance\"\"\"\n",
    "    vidcap = cv2.VideoCapture(vid_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"FPS of the video: {}\".formatfps)\n",
    "    success, frame = vidcap.read()\n",
    "    encodings = []\n",
    "    n_frame = 0\n",
    "    while success:\n",
    "        #TODO\n",
    "        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        boxes = face_recognition.face_locations()\n",
    "        success, frame = vidcap.read()\n",
    "#         if success:\n",
    "#             sim = similarity(frame1, frame2)\n",
    "#             sims.append(sim)\n",
    "#             print(\"At frame {}: similarity = {}\".format(n_frame, sim))\n",
    "#             frame1 = frame2\n",
    "            n_frame += 1\n",
    "    return encodings, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFaces(image, model=\"cnn\"):\n",
    "    \"\"\"Returns b-boxes and encodings of all faces present in the image\n",
    "    \n",
    "    :param image: must be in RGB format\n",
    "    :param model: hog is less accurate but faster compared to cnn\"\"\"\n",
    "    boxes = face_recognition.face_locations(image,model=model)\n",
    "    encodings = face_recognition.face_encodings(image,boxes)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(162, 154, 230, 85), (109, 360, 149, 321), (117, 556, 157, 517)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findFaces(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(frame1, frame2):\n",
    "    \"\"\"Returns SSIM similarity between two images\"\"\"\n",
    "    #s = measure.compare_mse(frame1, frame2)\n",
    "    s = measure.compare_ssim(frame1, frame2, multichannel=True)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(vid_path):\n",
    "    \"\"\"Returns list of similarities of consecutive frames in the video and its fps\"\"\"\n",
    "    vidcap = cv2.VideoCapture(vid_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"FPS of the video: {}\".formatfps)\n",
    "    success, frame1 = vidcap.read()\n",
    "    sims = []\n",
    "    count = 0\n",
    "    while success:\n",
    "        success, frame2 = vidcap.read()\n",
    "        if success:\n",
    "            sim = similarity(frame1, frame2)\n",
    "            sims.append(sim)\n",
    "            print(\"At frame {}: similarity = {}\".format(count, sim))\n",
    "            frame1 = frame2\n",
    "            count += 1\n",
    "    #Plotting time vs similarities\n",
    "    plt.plot([x/fps for x in range(count)], sims)\n",
    "    plt.show()\n",
    "    return sims, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sims,_ = similarities(\"../data/SAA_clip.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sims_inv = [1-x for x in sims] # Difference = 1 - similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x/fps for x in range(len(sims))], sims_inv) #Video time vs frame_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySceneDetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_milsec(timestr):\n",
    "    \"\"\"Converts scenedetect's output time string into milliseconds\"\"\"\n",
    "    HMS,MSEC = timestr.split('.')\n",
    "    x = time.strptime(HMS,'%H:%M:%S')\n",
    "    seconds = datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n",
    "    return seconds*1000 + int(MSEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_boundaries(vid_path):\n",
    "    \"\"\"Returns list of shot boundaries in video_time\"\"\"\n",
    "    file_name = os.path.basename(vid_path).split('.')[0]\n",
    "    command = \"scenedetect -i {} -s {}.stats.csv detect-content list-scenes\".format(vid_path,file_name)\n",
    "    os.system(command)\n",
    "    results = pd.read_csv(\"{}-Scenes.csv\".format(file_name))\n",
    "    results = results.columns.tolist()[1:] #List of timestamps of shot boundaries\n",
    "    shot_bounds = [time_to_milsec(x) for x in results]\n",
    "    return shot_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_frames(shot_bounds, vid_path, store = False):\n",
    "    \"\"\"Returns list of key_frames in numpy array format, one for each shot \n",
    "       and the timestamps of these key_frames\"\"\"\n",
    "    # Taking centre frame of each shot - creating a list of such \n",
    "    # centre frames from the extracted shot boundaries\n",
    "    file_name = os.path.basename(vid_path).split('.')[0]\n",
    "    key_frames = []\n",
    "    timestamps = []\n",
    "    bound1 = 0\n",
    "    vidcap = cv2.VideoCapture(vid_path)\n",
    "    for i in tqdm(range(len(shot_bounds) + 1)):\n",
    "        \n",
    "        if(i != len(shot_bounds)): #not last boundary\n",
    "            bound2 = shot_bounds[i]\n",
    "            \n",
    "        else: #last boundary\n",
    "            vidcap.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "            bound2 = vidcap.get(cv2.CAP_PROP_POS_MSEC) #max duration\n",
    "            \n",
    "        key_frame_msec = (bound1 + bound2)/2 #Average of 2 boundaries\n",
    "        timestamps.append(key_frame_msec)\n",
    "        #print(key_frame_msec)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,key_frame_msec)\n",
    "        _,image = vidcap.read() #Reading frame at key_frame_sec\n",
    "        if (store==True):\n",
    "            if not (os.path.isdir(file_name+'_key_frames')):\n",
    "                os.mkdir(file_name+'_key_frames')\n",
    "            cv2.imwrite(\"{}/frame_{}_{}.jpg\".format(file_name+'_key_frames',i,key_frame_msec),image)\n",
    "        key_frames.append(image) #storing frame as np array\n",
    "        bound1 = bound2\n",
    "        \n",
    "    return key_frames,timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_key_frames(vid_path):\n",
    "    \"\"\"Stores key frames of the video in a new directory\"\"\"\n",
    "    shot_bounds = shot_boundaries(vid_path)\n",
    "    return shot_bounds, get_key_frames(shot_bounds,vid_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = store_key_frames(\"../data/SAA_clip.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagecluster import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.main('2006_clipped_key_frames/',sim=0.65,vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree SAA_clip_key_frames/imagecluster/clusters/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scenes(vid_path, threshold):\n",
    "    \"\"\"Groups shots into scenes\"\"\"\n",
    "    print('Finding shot boundaries...')\n",
    "    shot_bounds = shot_boundaries(vid_path)\n",
    "    print('Extracting key frames...')\n",
    "    key_frames, kf_timestamps = get_key_frames(shot_bounds,vid_path)\n",
    "    #print(key_frames)\n",
    "    scene_count = 0\n",
    "    n_shots = len(key_frames)\n",
    "    scenes = [0] #What scene is each shot - length = no. of shots [Maps shots to scenes]\n",
    "    key_shots = [0] #Key shot for each scene [Maps scenes to their key shots]\n",
    "    print('Classifying shots into scenes...')\n",
    "    for i in tqdm(range(1, n_shots)): #For each shot\n",
    "        curr_shot = key_frames[i]\n",
    "        found = False\n",
    "        for key_shot in key_shots[-1:-16:-1]: #Iterate through key_shots of last few scenes - param\n",
    "            if (similarity(key_frames[key_shot],curr_shot) > threshold):\n",
    "                found = True\n",
    "                scenes.append(scenes[key_shot]) #Mark the shot as belonging to this scene\n",
    "                break\n",
    "        if (found == False): #End of scenes => No matching scene is found\n",
    "            scene_count += 1 \n",
    "            scenes.append(scene_count) #This shot belongs to the new scene\n",
    "            key_shots.append(i) #Mark this shot as key shot of the new scene\n",
    "    return (scenes, key_shots, scene_count, kf_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = find_scenes(\"../../2006-01-02_0000_US_00001057_V11_M2_VHS10_H4_JA.mp4\", 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ping_telegram('Finding scenes done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling data as a binary stream\n",
    "file = open('dump_binary','wb')\n",
    "pickle.dump(output, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pickle\n",
    "infile = open('dump_binary','rb')\n",
    "pp = pickle.load(infile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

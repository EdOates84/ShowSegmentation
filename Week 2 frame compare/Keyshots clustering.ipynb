{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ping_telegram(text):\n",
    "    import telegram\n",
    "    bot = telegram.Bot(token=\"848617644:AAH-YU71Klu7amhz0wtVBto0ACxhDhvYTaE\")\n",
    "    bot.send_message(chat_id=\"625772042\", text = text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(frame1, frame2):\n",
    "    \"\"\"Returns SSIM similarity between two images\"\"\"\n",
    "    #s = measure.compare_mse(frame1, frame2)\n",
    "    s = measure.compare_ssim(frame1, frame2, multichannel=True)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(vid_path):\n",
    "    \"\"\"Returns list of similarities of consecutive frames in the video and its fps\"\"\"\n",
    "    vidcap = cv2.VideoCapture(vid_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"FPS of the video: {}\".formatfps)\n",
    "    vidcap.set\n",
    "    success, frame1 = vidcap.read()\n",
    "    sims = []\n",
    "    count = 0\n",
    "    while success:\n",
    "        success, frame2 = vidcap.read()\n",
    "        if success:\n",
    "            sim = similarity(frame1, frame2)\n",
    "            sims.append(sim)\n",
    "            print(\"At frame {}: similarity = {}\".format(count, sim))\n",
    "            frame1 = frame2\n",
    "            count += 1\n",
    "    #Plotting time vs similarities\n",
    "    plt.plot([x/fps for x in range(count)], sims)\n",
    "    plt.show()\n",
    "    return sims, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sims,_ = similarities(\"../data/SAA_clip.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sims_inv = [1-x for x in sims] # Difference = 1 - similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x/fps for x in range(len(sims))], sims_inv) #Video time vs frame_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySceneDetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_milsec(timestr):\n",
    "    \"\"\"Converts scenedetect's output time string into milliseconds\"\"\"\n",
    "    HMS,MSEC = timestr.split('.')\n",
    "    x = time.strptime(HMS,'%H:%M:%S')\n",
    "    seconds = datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n",
    "    return seconds*1000 + int(MSEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_boundaries(vid_path):\n",
    "    \"\"\"Returns list of shot boundaries in video_time\"\"\"\n",
    "    file_name = os.path.basename(vid_path).split('.')[0]\n",
    "    command = \"scenedetect -i {} -s {}.stats.csv detect-content list-scenes\".format(vid_path,file_name)\n",
    "    os.system(command)\n",
    "    results = pd.read_csv(\"{}-Scenes.csv\".format(file_name))\n",
    "    results = results.columns.tolist()[1:] #List of timestamps of shot boundaries\n",
    "    shot_bounds = [time_to_milsec(x) for x in results]\n",
    "    return shot_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_frames(shot_bounds, vid_path, store = False):\n",
    "    \"\"\"Returns list of key_frames in numpy array format, one for each shot \n",
    "       and the timestamps of these key_frames\"\"\"\n",
    "    # Taking centre frame of each shot - creating a list of such \n",
    "    # centre frames from the extracted shot boundaries\n",
    "    file_name = os.path.basename(vid_path).split('.')[0]\n",
    "    key_frames = []\n",
    "    timestamps = []\n",
    "    bound1 = 0\n",
    "    vidcap = cv2.VideoCapture(vid_path)\n",
    "    for i in tqdm(range(len(shot_bounds) + 1)):\n",
    "        \n",
    "        if(i != len(shot_bounds)): #not last boundary\n",
    "            bound2 = shot_bounds[i]\n",
    "            \n",
    "        else: #last boundary\n",
    "            vidcap.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "            bound2 = vidcap.get(cv2.CAP_PROP_POS_MSEC) #max duration\n",
    "            \n",
    "        key_frame_msec = (bound1 + bound2)/2 #Average of 2 boundaries\n",
    "        timestamps.append(key_frame_msec)\n",
    "        #print(key_frame_msec)\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,key_frame_msec)\n",
    "        _,image = vidcap.read() #Reading frame at key_frame_sec\n",
    "        if (store==True):\n",
    "            if not (os.path.isdir(file_name+'_key_frames')):\n",
    "                os.mkdir(file_name+'_key_frames')\n",
    "            cv2.imwrite(\"{}/frame_{}_{}.jpg\".format(file_name+'_key_frames',i,key_frame_msec),image)\n",
    "        key_frames.append(image) #storing frame as np array\n",
    "        bound1 = bound2\n",
    "        \n",
    "    return key_frames,timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_key_frames(vid_path):\n",
    "    \"\"\"Stores key frames of the video in a new directory\"\"\"\n",
    "    shot_bounds = shot_boundaries(vid_path)\n",
    "    return shot_bounds, get_key_frames(shot_bounds,vid_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:03<00:00, 22.85it/s]\n"
     ]
    }
   ],
   "source": [
    "op = store_key_frames(\"../data/SAA_clip.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imagecluster import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fingerprints database 2006_clipped_key_frames/imagecluster/fingerprints.pk ...\n",
      "clustering ...\n",
      "#images : #clusters\n",
      "2 : 75\n",
      "3 : 24\n",
      "4 : 12\n",
      "5 : 4\n",
      "6 : 4\n",
      "7 : 5\n",
      "8 : 4\n",
      "9 : 3\n",
      "10 : 3\n",
      "11 : 1\n",
      "13 : 3\n",
      "14 : 1\n",
      "15 : 1\n",
      "16 : 1\n",
      "18 : 1\n",
      "21 : 1\n",
      "25 : 2\n",
      "26 : 1\n",
      "35 : 1\n",
      "#images in clusters total:  683\n",
      "cluster dir: 2006_clipped_key_frames/imagecluster/clusters\n",
      "plot array (uint8) size: 738.5888671875 MiB\n"
     ]
    }
   ],
   "source": [
    "main.main('2006_clipped_key_frames/',sim=0.65,vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mSAA_clip_key_frames/imagecluster/clusters/\u001b[00m\r\n",
      "├── \u001b[01;34mcluster_with_11\u001b[00m\r\n",
      "│   └── \u001b[01;34mcluster_0\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_0_4721.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_0_4721.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_10_102018.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_10_102018.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_1_10961.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_1_10961.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_2_13813.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_2_13813.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_3_16499.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_3_16499.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_4_22839.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_4_22839.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_5_36103.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_5_36103.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_6_47748.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_6_47748.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_7_57374.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_7_57374.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_8_77193.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_8_77193.5.jpg\u001b[00m\r\n",
      "│       └── \u001b[01;36mframe_9_95879.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_9_95879.0.jpg\u001b[00m\r\n",
      "├── \u001b[01;34mcluster_with_2\u001b[00m\r\n",
      "│   ├── \u001b[01;34mcluster_0\u001b[00m\r\n",
      "│   │   ├── \u001b[01;36mframe_21_166833.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_21_166833.5.jpg\u001b[00m\r\n",
      "│   │   └── \u001b[01;36mframe_53_219319.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_53_219319.0.jpg\u001b[00m\r\n",
      "│   ├── \u001b[01;34mcluster_1\u001b[00m\r\n",
      "│   │   ├── \u001b[01;36mframe_26_178261.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_26_178261.5.jpg\u001b[00m\r\n",
      "│   │   └── \u001b[01;36mframe_27_179162.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_27_179162.5.jpg\u001b[00m\r\n",
      "│   ├── \u001b[01;34mcluster_2\u001b[00m\r\n",
      "│   │   ├── \u001b[01;36mframe_67_237120.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_67_237120.5.jpg\u001b[00m\r\n",
      "│   │   └── \u001b[01;36mframe_69_238488.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_69_238488.5.jpg\u001b[00m\r\n",
      "│   ├── \u001b[01;34mcluster_3\u001b[00m\r\n",
      "│   │   ├── \u001b[01;36mframe_57_227461.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_57_227461.0.jpg\u001b[00m\r\n",
      "│   │   └── \u001b[01;36mframe_73_241541.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_73_241541.5.jpg\u001b[00m\r\n",
      "│   └── \u001b[01;34mcluster_4\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_43_202502.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_43_202502.5.jpg\u001b[00m\r\n",
      "│       └── \u001b[01;36mframe_63_233467.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_63_233467.0.jpg\u001b[00m\r\n",
      "├── \u001b[01;34mcluster_with_23\u001b[00m\r\n",
      "│   └── \u001b[01;34mcluster_0\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_12_110493.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_12_110493.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_13_117317.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_13_117317.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_22_170053.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_22_170053.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_23_171405.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_23_171405.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_24_173306.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_24_173306.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_29_182499.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_29_182499.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_30_183516.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_30_183516.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_33_186353.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_33_186353.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_34_187804.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_34_187804.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_35_189389.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_35_189389.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_36_191107.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_36_191107.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_39_195929.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_39_195929.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_40_197598.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_40_197598.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_41_198899.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_41_198899.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_42_200700.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_42_200700.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_44_203704.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_44_203704.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_46_206556.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_46_206556.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_47_208125.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_47_208125.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_51_214047.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_51_214047.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_52_215048.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_52_215048.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_54_223740.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_54_223740.5.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_56_226276.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_56_226276.5.jpg\u001b[00m\r\n",
      "│       └── \u001b[01;36mframe_59_229346.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_59_229346.0.jpg\u001b[00m\r\n",
      "├── \u001b[01;34mcluster_with_3\u001b[00m\r\n",
      "│   ├── \u001b[01;34mcluster_0\u001b[00m\r\n",
      "│   │   ├── \u001b[01;36mframe_31_184317.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_31_184317.5.jpg\u001b[00m\r\n",
      "│   │   ├── \u001b[01;36mframe_32_185068.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_32_185068.5.jpg\u001b[00m\r\n",
      "│   │   └── \u001b[01;36mframe_45_205005.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_45_205005.0.jpg\u001b[00m\r\n",
      "│   └── \u001b[01;34mcluster_1\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_37_192459.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_37_192459.0.jpg\u001b[00m\r\n",
      "│       ├── \u001b[01;36mframe_38_194027.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_38_194027.5.jpg\u001b[00m\r\n",
      "│       └── \u001b[01;36mframe_49_211394.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_49_211394.5.jpg\u001b[00m\r\n",
      "└── \u001b[01;34mcluster_with_4\u001b[00m\r\n",
      "    ├── \u001b[01;34mcluster_0\u001b[00m\r\n",
      "    │   ├── \u001b[01;36mframe_11_103670.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_11_103670.0.jpg\u001b[00m\r\n",
      "    │   ├── \u001b[01;36mframe_55_225075.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_55_225075.5.jpg\u001b[00m\r\n",
      "    │   ├── \u001b[01;36mframe_61_231164.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_61_231164.5.jpg\u001b[00m\r\n",
      "    │   └── \u001b[01;36mframe_66_236586.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_66_236586.5.jpg\u001b[00m\r\n",
      "    └── \u001b[01;34mcluster_1\u001b[00m\r\n",
      "        ├── \u001b[01;36mframe_14_119953.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_14_119953.0.jpg\u001b[00m\r\n",
      "        ├── \u001b[01;36mframe_15_124224.0.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_15_124224.0.jpg\u001b[00m\r\n",
      "        ├── \u001b[01;36mframe_16_130680.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_16_130680.5.jpg\u001b[00m\r\n",
      "        └── \u001b[01;36mframe_19_154220.5.jpg\u001b[00m -> \u001b[01;35m/home/eon/Desktop/ShowSegmentation/Week 2 frame compare/SAA_clip_key_frames/frame_19_154220.5.jpg\u001b[00m\r\n",
      "\r\n",
      "16 directories, 58 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree SAA_clip_key_frames/imagecluster/clusters/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scenes(vid_path, threshold):\n",
    "    \"\"\"Groups shots into scenes\"\"\"\n",
    "    print('Finding shot boundaries...')\n",
    "    shot_bounds = shot_boundaries(vid_path)\n",
    "    print('Extracting key frames...')\n",
    "    key_frames, kf_timestamps = get_key_frames(shot_bounds,vid_path)\n",
    "    #print(key_frames)\n",
    "    scene_count = 0\n",
    "    n_shots = len(key_frames)\n",
    "    scenes = [0] #What scene is each shot - length = no. of shots [Maps shots to scenes]\n",
    "    key_shots = [0] #Key shot for each scene [Maps scenes to their key shots]\n",
    "    print('Classifying shots into scenes...')\n",
    "    for i in tqdm(range(1, n_shots)): #For each shot\n",
    "        curr_shot = key_frames[i]\n",
    "        found = False\n",
    "        for key_shot in key_shots[-1:-16:-1]: #Iterate through key_shots of last few scenes - param\n",
    "            if (similarity(key_frames[key_shot],curr_shot) > threshold):\n",
    "                found = True\n",
    "                scenes.append(scenes[key_shot]) #Mark the shot as belonging to this scene\n",
    "                break\n",
    "        if (found == False): #End of scenes => No matching scene is found\n",
    "            scene_count += 1 \n",
    "            scenes.append(scene_count) #This shot belongs to the new scene\n",
    "            key_shots.append(i) #Mark this shot as key shot of the new scene\n",
    "    return (scenes, key_shots, scene_count, kf_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding shot boundaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eon/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2961: DtypeWarning: Columns (0,1,3,4,6,7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting key frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4731/4731 [04:35<00:00, 17.20it/s]\n",
      "  0%|          | 0/4730 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying shots into scenes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eon/anaconda3/lib/python3.7/site-packages/skimage/util/arraycrop.py:177: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  cropped = ar[slices]\n",
      "100%|██████████| 4730/4730 [1:41:52<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "output = find_scenes(\"../../2006-01-02_0000_US_00001057_V11_M2_VHS10_H4_JA.mp4\", 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ping_telegram('Finding scenes done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling data as a binary stream\n",
    "file = open('dump_binary','wb')\n",
    "pickle.dump(output, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pickle\n",
    "infile = open('dump_binary','rb')\n",
    "pp = pickle.load(infile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
